wandb: Currently logged in as: esarai (esarai-universit-de-moncton). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.22.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.9
wandb: Run data is saved locally in /project/6088306/sarai/CrossPoint_OF2/wandb/run-20251024_102750-gtovus9m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run of2_full_run
wandb: â­ï¸ View project at https://wandb.ai/esarai-universit-de-moncton/CrossPoint-OF2
wandb: ğŸš€ View run at https://wandb.ai/esarai-universit-de-moncton/CrossPoint-OF2/runs/gtovus9m
Using cache found in /home/sarai/.cache/torch/hub/pytorch_vision_v0.10.0
/home/sarai/jupyter1/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/sarai/jupyter1/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/sarai/jupyter1/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
/home/sarai/jupyter1/lib/python3.10/site-packages/wandb/wandb_torch.py:193: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /shared_tmp/build_wheels_tmp.115664/python-3.10/torch/torch/csrc/tensor/python_tensor.cpp:78.)
  check = torch.cuda.FloatTensor(1).fill_(0)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      Epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: Train Loss â–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      Epoch 99
wandb: Train Loss 0.26096
wandb: 
wandb: ğŸš€ View run of2_full_run at: https://wandb.ai/esarai-universit-de-moncton/CrossPoint-OF2/runs/gtovus9m
wandb: ï¸âš¡ View job at https://wandb.ai/esarai-universit-de-moncton/CrossPoint-OF2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjcwMDIwNjU0NA==/version_details/v4
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251024_102750-gtovus9m/logs
